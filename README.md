# LLaMA 3.2: Advanced Language Model Architecture and Applications

## Presentation Materials

1. [Overview](presentation-materials/00_overview.md)
2. [Architecture Guide](presentation-materials/01_Llama-3.2-Architecture-Guide.md)
3. [Llama Stack](presentation-materials/02_llama-stack.md)
4. [Limitations](presentation-materials/03_limitations.md)

## Colab Notebook
[Notebook Link](https://colab.research.google.com/drive/1AKB1lHl07uQwUArNZ7On5KG_Atshmuyj#scrollTo=a3CvtqEOyYCu)

## Additional Resources

### Research Papers

1. Touvron, H., et al. (2023). "LLaMA: Open and Efficient Foundation Language Models." arXiv preprint arXiv:2302.13971.

2. Hoffmann, J., et al. (2022). "Training Compute-Optimal Large Language Models." arXiv preprint arXiv:2203.15556.

3. Brown, T. B., et al. (2020). "Language Models are Few-Shot Learners." arXiv preprint arXiv:2005.14165.

### Online Courses and Tutorials

1. [Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning) by Andrew Ng on Coursera

2. [Hugging Face Transformers Course](https://huggingface.co/course/chapter1/1)

### Books

1. Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.

2. Jurafsky, D., & Martin, J. H. (2020). Speech and Language Processing (3rd ed. draft). 

3. Vaswani, A., et al. (2017). "Attention Is All You Need." In Advances in Neural Information Processing Systems.
### Tools and Libraries

1. [PyTorch](https://pytorch.org/) - Open source machine learning library

2. [Transformers](https://github.com/huggingface/transformers) - State-of-the-art Natural Language Processing for PyTorch and TensorFlow 2.0

3. [Datasets](https://huggingface.co/docs/datasets/) - Library for easily accessing and sharing datasets

4. [Gradio](https://gradio.app/) - Create UIs for your machine learning models

5. [unsloth](https://github.com/unslothai/unsloth) - Efficient fine-tuning of LLMs with optimized CUDA kernels

### Blogs and Websites

1. [Meta AI Blog](https://ai.meta.com/blog/) - Official blog covering LLaMA and other Meta AI research

2. [Llama](https://llama.com) - Official website for the LLaMA model family

3. [Hugging Face LLaMA Hub](https://huggingface.co/models?search=llama) - Collection of LLaMA models and weights

4. [Unsloth](https://github.com/unslothai/unsloth) - Efficient LLaMA fine-tuning with optimized CUDA kernels

5. [Ollama](https://ollama.ai/) - Run LLaMA and other open-source models locally


## Conclusion

This repository serves as a comprehensive resource for graduate-level Data Science students interested in understanding and working with advanced language models like LLaMA 3.2. The presentation materials provide an in-depth look at the model's architecture, capabilities, and limitations, while the additional resources offer a broader context for understanding the field of natural language processing and deep learning.

I encourage everyone to explore the provided materials, engage with the research papers, and experiment with the code examples to gain a deeper understanding of large language models and their applications in various domains of artificial intelligence and data science.